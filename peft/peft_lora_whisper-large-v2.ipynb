{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c219841f-493c-40f9-a6c9-3700f0c525d0",
   "metadata": {},
   "source": [
    "# PEFT åº“ LoRA å®æˆ˜ - OpenAI Whisper-large-v2\n",
    "\n",
    "æœ¬æ•™ç¨‹ä½¿ç”¨ LoRA åœ¨`OpenAI Whisper-large-v2`æ¨¡å‹ä¸Šå®ç°`è¯­éŸ³è¯†åˆ«(ASR)`ä»»åŠ¡çš„å¾®è°ƒè®­ç»ƒã€‚\n",
    "\n",
    "åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜ç»“åˆäº†`int8` é‡åŒ–è¿›ä¸€æ­¥é™ä½è®­ç»ƒè¿‡ç¨‹èµ„æºå¼€é”€ï¼ŒåŒæ—¶ä¿è¯äº†ç²¾åº¦å‡ ä¹ä¸å—å½±å“ã€‚\n",
    "\n",
    "![LoRA](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_diagram.png)\n",
    "\n",
    "æœ¬æ•™ç¨‹ä¸»è¦è®­ç»ƒæµç¨‹å¦‚ä¸‹ï¼š\n",
    "- å…¨å±€å‚æ•°è®¾ç½®\n",
    "- æ•°æ®å‡†å¤‡\n",
    "    - ä¸‹è½½æ•°æ®é›†ï¼šè®­ç»ƒã€éªŒè¯å’Œè¯„ä¼°é›†\n",
    "    - é¢„å¤„ç†æ•°æ®ï¼šé™é‡‡æ ·ã€ç§»é™¤ä¸å¿…è¦å­—æ®µç­‰\n",
    "    - æ•°æ®æŠ½æ ·ï¼ˆæ¼”ç¤ºéœ€è¦ï¼‰\n",
    "    - åº”ç”¨æ•°æ®é›†å¤„ç†ï¼ˆ`Dataset.map`ï¼‰\n",
    "    - è‡ªå®šä¹‰è¯­éŸ³æ•°æ®å¤„ç†å™¨\n",
    "- æ¨¡å‹å‡†å¤‡\n",
    "    - åŠ è½½å’Œå¤„ç† `int8` ç²¾åº¦ Whisper-Large-v2 æ¨¡å‹\n",
    "    - LoRA Adapter å‚æ•°é…ç½®\n",
    "    - å®ä¾‹åŒ– PEFT Modelï¼š`peft_model = get_peft_model(model, config)`\n",
    "- æ¨¡å‹è®­ç»ƒ\n",
    "    - è®­ç»ƒå‚æ•°é…ç½® Seq2SeqTrainingArguments\n",
    "    - å®ä¾‹åŒ–è®­ç»ƒå™¨ Seq2SeqTrainer\n",
    "    - è®­ç»ƒæ¨¡å‹\n",
    "    - ä¿å­˜æ¨¡å‹\n",
    "- æ¨¡å‹æ¨ç†\n",
    "    - ä½¿ç”¨ `PeftModel` åŠ è½½ LoRA å¾®è°ƒå Whisper æ¨¡å‹\n",
    "    - ä½¿ç”¨ `Pipeline API` éƒ¨ç½²å¾®è°ƒå Whisper å®ç°ä¸­æ–‡è¯­éŸ³è¯†åˆ«ä»»åŠ¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a1e23-ea71-45d6-82d6-453077cf2d29",
   "metadata": {},
   "source": [
    "## å…¨å±€å‚æ•°è®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccd00402-d821-485e-8703-fb16bcb56a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"openai/whisper-large-v2\"\n",
    "model_dir = \"models/whisper-large-v2-asr-int8\"\n",
    "\n",
    "language = \"Chinese (China)\"\n",
    "language_abbr = \"zh-CN\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"mozilla-foundation/common_voice_11_0\"\n",
    "\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfffa1df-e51e-4026-9817-1cebddf0061a",
   "metadata": {},
   "source": [
    "## æ•°æ®å‡†å¤‡\n",
    "\n",
    "### ä¸‹è½½æ•°æ®é›† Common Voice\n",
    "\n",
    "Common Voice 11.0 æ•°æ®é›†åŒ…å«è®¸å¤šä¸åŒè¯­è¨€çš„å½•éŸ³ï¼Œæ€»æ—¶é•¿è¾¾æ•°å°æ—¶ã€‚\n",
    "\n",
    "æœ¬æ•™ç¨‹ä»¥ä¸­æ–‡æ•°æ®ä¸ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨ LoRA åœ¨ Whisper-large-v2 ä¸Šè¿›è¡Œå¾®è°ƒè®­ç»ƒã€‚\n",
    "\n",
    "é¦–å…ˆï¼Œåˆå§‹åŒ–ä¸€ä¸ªDatasetDictç»“æ„ï¼Œå¹¶å°†è®­ç»ƒé›†ï¼ˆå°†è®­ç»ƒ+éªŒè¯æ‹†åˆ†ä¸ºè®­ç»ƒé›†ï¼‰å’Œæµ‹è¯•é›†æ‹†åˆ†å¥½ï¼ŒæŒ‰ç…§ä¸­æ–‡æ•°æ®é›†æ„å»ºé…ç½®åŠ è½½åˆ°å†…å­˜ä¸­ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ff42f4-f3ec-46d3-b0c0-dd9ffbf7b50b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client_id': '95368aab163e0387e4fd4991b4f2d8ccfbd4364bf656c860230501fd27dcedf087773e4695a6cf5de9c4f1d406d582283190d065cdfa36b0e2b060cffaca977e',\n",
       " 'path': 'C:\\\\Users\\\\kdgc\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\dc00ac467ed91488da8cd6425c8153ee8670d95921bbb4a42342990b3d7ae70c\\\\zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       " 'audio': {'path': 'C:\\\\Users\\\\kdgc\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\dc00ac467ed91488da8cd6425c8153ee8670d95921bbb4a42342990b3d7ae70c\\\\zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       "  'array': array([-9.09494702e-13, -2.50111043e-12, -2.04636308e-12, ...,\n",
       "          1.21667417e-05,  3.23003815e-06, -2.43064278e-07]),\n",
       "  'sampling_rate': 48000},\n",
       " 'sentence': 'æ€§å–œæ¸©æš–æ¶¦æ¹¿æ°”å€™ä¸”è€å¯’ã€‚',\n",
       " 'up_votes': 2,\n",
       " 'down_votes': 0,\n",
       " 'age': '',\n",
       " 'gender': '',\n",
       " 'accent': '',\n",
       " 'locale': 'zh-CN',\n",
       " 'segment': ''}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(dataset_name, language_abbr, split=\"train\", trust_remote_code=True)\n",
    "common_voice[\"validation\"] = load_dataset(dataset_name, language_abbr, split=\"validation\", trust_remote_code=True)\n",
    "\n",
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bed4735-d485-435f-b282-2806241e0e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "        num_rows: 29056\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "        num_rows: 10581\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81faa4-d8fe-4cc7-afe6-4c2615b9050f",
   "metadata": {},
   "source": [
    "## é¢„å¤„ç†è®­ç»ƒæ•°æ®é›†\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5822025f-7f8e-4141-8bfe-d8822d0da20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoTokenizer, AutoProcessor\n",
    "\n",
    "# ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ç‰¹å¾æå–å™¨\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "# ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½åˆ†è¯å™¨ï¼Œå¯ä»¥æŒ‡å®šè¯­è¨€å’Œä»»åŠ¡ä»¥è·å¾—æœ€é€‚åˆç‰¹å®šéœ€æ±‚çš„åˆ†è¯å™¨é…ç½®\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "\n",
    "# ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å¤„ç†å™¨ï¼Œå¤„ç†å™¨é€šå¸¸ç»“åˆäº†ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨ï¼Œä¸ºç‰¹å®šä»»åŠ¡æä¾›ä¸€ç«™å¼çš„æ•°æ®é¢„å¤„ç†\n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f394e5cd-23b8-413e-8bde-88c3542b84fa",
   "metadata": {},
   "source": [
    "#### ç§»é™¤æ•°æ®é›†ä¸­ä¸å¿…è¦çš„å­—æ®µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1690dc5a-c1f7-4556-9be3-d31ad888e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = common_voice.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "309aff16-ea26-4474-af54-7ef244783999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': 'C:\\\\Users\\\\kdgc\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\dc00ac467ed91488da8cd6425c8153ee8670d95921bbb4a42342990b3d7ae70c\\\\zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       "  'array': array([-9.09494702e-13, -2.50111043e-12, -2.04636308e-12, ...,\n",
       "          1.21667417e-05,  3.23003815e-06, -2.43064278e-07]),\n",
       "  'sampling_rate': 48000},\n",
       " 'sentence': 'æ€§å–œæ¸©æš–æ¶¦æ¹¿æ°”å€™ä¸”è€å¯’ã€‚'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881546ab-72e4-4bcf-852f-a8be736164b7",
   "metadata": {},
   "source": [
    "#### é™é‡‡æ ·éŸ³é¢‘æ•°æ®\n",
    "\n",
    "æŸ¥çœ‹`common_voice` æ•°æ®é›†ä»‹ç»ï¼Œä½ ä¼šå‘ç°å…¶éŸ³é¢‘æ˜¯ä»¥48kHzçš„é‡‡æ ·ç‡è¿›è¡Œé‡‡æ ·çš„.\n",
    "\n",
    "è€Œ`Whisper`æ¨¡å‹æ˜¯åœ¨16kHZçš„éŸ³é¢‘è¾“å…¥ä¸Šé¢„è®­ç»ƒçš„ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å°†éŸ³é¢‘è¾“å…¥é™é‡‡æ ·ä»¥åŒ¹é…æ¨¡å‹é¢„è®­ç»ƒæ—¶ä½¿ç”¨çš„é‡‡æ ·ç‡ã€‚\n",
    "\n",
    "é€šè¿‡åœ¨éŸ³é¢‘åˆ—ä¸Šä½¿ç”¨`cast_column`æ–¹æ³•ï¼Œå¹¶å°†`sampling_rate`è®¾ç½®ä¸º16kHzæ¥å¯¹éŸ³é¢‘è¿›è¡Œé™é‡‡æ ·ã€‚\n",
    "\n",
    "ä¸‹æ¬¡è°ƒç”¨æ—¶ï¼ŒéŸ³é¢‘è¾“å…¥å°†å®æ—¶é‡æ–°å–æ ·ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fc451cc-e21e-473c-a702-d7d6ed098f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc3d7fcc-7c34-41c8-9857-5a6e883f6115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': 'C:\\\\Users\\\\kdgc\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\dc00ac467ed91488da8cd6425c8153ee8670d95921bbb4a42342990b3d7ae70c\\\\zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       "  'array': array([ 5.82076609e-11, -2.91038305e-11, -5.82076609e-11, ...,\n",
       "         -5.96660539e-06,  2.71383760e-05,  1.29687833e-05]),\n",
       "  'sampling_rate': 16000},\n",
       " 'sentence': 'æ€§å–œæ¸©æš–æ¶¦æ¹¿æ°”å€™ä¸”è€å¯’ã€‚'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling_rate ä» 48KHZ é™ä¸º 16KHZ\n",
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55908f-3ea3-4aee-8062-6f8d3a6573b9",
   "metadata": {},
   "source": [
    "### æ•´åˆä»¥ä¸Šæ•°æ®å¤„ç†ä¸ºä¸€ä¸ªå‡½æ•°\n",
    "\n",
    "è¯¥æ•°æ®é¢„å¤„ç†å‡½æ•°åº”è¯¥åŒ…æ‹¬ï¼š\n",
    "- é€šè¿‡åŠ è½½éŸ³é¢‘åˆ—å°†éŸ³é¢‘è¾“å…¥é‡æ–°é‡‡æ ·ä¸º16kHZã€‚\n",
    "- ä½¿ç”¨ç‰¹å¾æå–å™¨ä»éŸ³é¢‘æ•°ç»„è®¡ç®—è¾“å…¥ç‰¹å¾ã€‚\n",
    "- å°†å¥å­åˆ—æ ‡è®°åŒ–ä¸ºè¾“å…¥æ ‡ç­¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58f42c35-35ba-4d6b-9d15-095963cec67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a04c60-09be-419c-bdc6-6d56bbf9d4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8923262-f881-476f-9806-61a3c8fb8518",
   "metadata": {},
   "source": [
    "### æ•°æ®æŠ½æ ·ï¼ˆæ¼”ç¤ºéœ€è¦ï¼‰\n",
    "\n",
    "åœ¨ Whisper-Large-v2 ä¸Šä½¿ç”¨å°è§„æ¨¡æ•°æ®è¿›è¡Œæ¼”ç¤ºè®­ç»ƒï¼Œä¿æŒä»¥ä¸‹è®­ç»ƒå‚æ•°ä¸å˜ï¼ˆbatch_size=64ï¼‰ã€‚\n",
    "\n",
    "ä½¿ç”¨ 640 ä¸ªæ ·æœ¬è®­ç»ƒï¼Œ320ä¸ªæ ·æœ¬éªŒè¯å’Œè¯„ä¼°ï¼Œæ°å¥½ä½¿å¾—1ä¸ª epoch ä»…éœ€10 steps å³å¯å®Œæˆè®­ç»ƒã€‚\n",
    "\n",
    "ï¼ˆåœ¨ NVIDIA T4 ä¸Šéœ€è¦10-15åˆ†é’Ÿï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28b14693-aa42-4f13-a537-66b5c4cb4718",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_common_voice = DatasetDict()\n",
    "\n",
    "small_common_voice[\"train\"] = common_voice[\"train\"].shuffle(seed=16).select(range(640))\n",
    "small_common_voice[\"validation\"] = common_voice[\"validation\"].shuffle(seed=16).select(range(320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46b6a10b-81a9-428d-8130-4c2b626f3ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 640\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 320\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_common_voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40f480-6eeb-4546-891b-97ec4a7ec46d",
   "metadata": {},
   "source": [
    "### å¦‚æœå…¨é‡è®­ç»ƒï¼Œåˆ™ä½¿ç”¨å®Œæ•´æ•°æ®ä»£æ›¿æŠ½æ ·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "392f7856-a720-40a7-af7e-40e185fc315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŠ½æ ·æ•°æ®å¤„ç†\n",
    "tokenized_common_voice = small_common_voice.map(prepare_dataset)\n",
    "\n",
    "# å®Œæ•´æ•°æ®è®­ç»ƒï¼Œå°è¯•å¼€å¯ `num_proc=8` å‚æ•°å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†ï¼ˆå¦‚é˜»å¡æ— æ³•è¿è¡Œï¼Œåˆ™ä¸ä½¿ç”¨æ­¤å‚æ•°ï¼‰\n",
    "#tokenized_common_voice = common_voice.map(prepare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76646516-06e2-4700-92fe-4fbb87587e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sentence', 'input_features', 'labels'],\n",
       "        num_rows: 640\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'sentence', 'input_features', 'labels'],\n",
       "        num_rows: 320\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_common_voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ec184e-d840-40b6-99af-d11392273442",
   "metadata": {},
   "source": [
    "### è‡ªå®šä¹‰è¯­éŸ³æ•°æ®æ•´ç†å™¨\n",
    "\n",
    "å®šä¹‰äº†ä¸€ä¸ªé’ˆå¯¹è¯­éŸ³åˆ°æ–‡æœ¬ï¼ˆSeq2Seqï¼‰æ¨¡å‹çš„è‡ªå®šä¹‰æ•°æ®æ•´ç†å™¨ç±»ï¼Œç‰¹åˆ«é€‚ç”¨äºè¾“å…¥ä¸ºè¯­éŸ³ç‰¹å¾ã€è¾“å‡ºä¸ºæ–‡æœ¬åºåˆ—çš„æ•°æ®é›†ã€‚\n",
    "\n",
    "\n",
    "è¿™ä¸ªæ•´ç†å™¨ï¼ˆ`DataCollatorSpeechSeq2SeqWithPadding`ï¼‰æ—¨åœ¨å°†æ•°æ®ç‚¹æ‰¹é‡æ‰“åŒ…ï¼Œå°†æ¯ä¸ªæ‰¹æ¬¡ä¸­çš„`attention_mask`å¡«å……åˆ°æœ€å¤§é•¿åº¦ï¼Œä»¥ä¿æŒæ‰¹å¤„ç†ä¸­å¼ é‡å½¢çŠ¶çš„ä¸€è‡´æ€§ï¼Œå¹¶ç”¨`-100`æ›¿æ¢å¡«å……å€¼ï¼Œä»¥ä¾¿åœ¨æŸå¤±å‡½æ•°ä¸­è¢«å¿½ç•¥ã€‚è¿™å¯¹äºç¥ç»ç½‘ç»œçš„é«˜æ•ˆè®­ç»ƒè‡³å…³é‡è¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c89ffcf-c805-48c2-b7d3-ae01b687178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªé’ˆå¯¹è¯­éŸ³åˆ°æ–‡æœ¬ä»»åŠ¡çš„æ•°æ®æ•´ç†å™¨ç±»\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any  # å¤„ç†å™¨ç»“åˆäº†ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨\n",
    "\n",
    "    # æ•´ç†å™¨å‡½æ•°ï¼Œå°†ç‰¹å¾åˆ—è¡¨å¤„ç†æˆä¸€ä¸ªæ‰¹æ¬¡\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # ä»ç‰¹å¾åˆ—è¡¨ä¸­æå–è¾“å…¥ç‰¹å¾ï¼Œå¹¶å¡«å……ä»¥ä½¿å®ƒä»¬å…·æœ‰ç›¸åŒçš„å½¢çŠ¶\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # ä»ç‰¹å¾åˆ—è¡¨ä¸­æå–æ ‡ç­¾ç‰¹å¾ï¼ˆæ–‡æœ¬ä»¤ç‰Œï¼‰ï¼Œå¹¶è¿›è¡Œå¡«å……\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # ä½¿ç”¨-100æ›¿æ¢æ ‡ç­¾ä¸­çš„å¡«å……åŒºåŸŸï¼Œ-100é€šå¸¸ç”¨äºåœ¨æŸå¤±è®¡ç®—ä¸­å¿½ç•¥å¡«å……ä»¤ç‰Œ\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # å¦‚æœæ‰¹æ¬¡ä¸­çš„æ‰€æœ‰åºåˆ—éƒ½ä»¥å¥å­å¼€å§‹ä»¤ç‰Œå¼€å¤´ï¼Œåˆ™ç§»é™¤å®ƒ\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        # å°†å¤„ç†è¿‡çš„æ ‡ç­¾æ·»åŠ åˆ°æ‰¹æ¬¡ä¸­\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch  # è¿”å›æœ€ç»ˆçš„æ‰¹æ¬¡ï¼Œå‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒæˆ–è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a26a6b4d-5370-4a48-936a-84739ac0cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”¨ç»™å®šçš„å¤„ç†å™¨å®ä¾‹åŒ–æ•°æ®æ•´ç†å™¨\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ecd4bc-01fd-4286-afe5-fe2639ae15a1",
   "metadata": {},
   "source": [
    "## æ¨¡å‹å‡†å¤‡\n",
    "\n",
    "### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆint8 ç²¾åº¦ï¼‰\n",
    "\n",
    "ä½¿ç”¨ `int8 ` ç²¾åº¦åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿›ä¸€æ­¥é™ä½æ˜¾å­˜éœ€æ±‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9fcb121-fa5c-4c30-8bdc-9ab08ab75427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name_or_path, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cb016f1-e6e9-4fd8-9c8b-72fd23be92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®æ¨¡å‹é…ç½®ä¸­çš„forced_decoder_idså±æ€§ä¸ºNone\n",
    "model.config.forced_decoder_ids = None  # è¿™é€šå¸¸ç”¨äºæŒ‡å®šåœ¨è§£ç ï¼ˆç”Ÿæˆæ–‡æœ¬ï¼‰è¿‡ç¨‹ä¸­å¿…é¡»ä½¿ç”¨çš„ç‰¹å®štokençš„IDï¼Œè®¾ç½®ä¸ºNoneè¡¨ç¤ºæ²¡æœ‰è¿™æ ·çš„å¼ºåˆ¶è¦æ±‚\n",
    "\n",
    "# è®¾ç½®æ¨¡å‹é…ç½®ä¸­çš„suppress_tokensåˆ—è¡¨ä¸ºç©º\n",
    "model.config.suppress_tokens = []  # è¿™ç”¨äºæŒ‡å®šåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åº”è¢«æŠ‘åˆ¶ï¼ˆä¸ç”Ÿæˆï¼‰çš„tokençš„åˆ—è¡¨ï¼Œè®¾ç½®ä¸ºç©ºåˆ—è¡¨è¡¨ç¤ºæ²¡æœ‰è¦æŠ‘åˆ¶çš„token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba1fa0-ea15-48d9-8c16-70df9f0b60b1",
   "metadata": {},
   "source": [
    "### PEFT å¾®è°ƒå‰çš„æ¨¡å‹å¤„ç†\n",
    "\n",
    "åœ¨ä½¿ç”¨ `peft` è®­ç»ƒ int8 æ¨¡å‹ä¹‹å‰ï¼Œéœ€è¦è¿›è¡Œä¸€äº›é¢„å¤„ç†ï¼š\n",
    "- å°†æ‰€æœ‰é `int8` ç²¾åº¦æ¨¡å—è½¬æ¢ä¸ºå…¨ç²¾åº¦ï¼ˆ`fp32`ï¼‰ä»¥ä¿è¯ç¨³å®šæ€§\n",
    "- ä¸ºè¾“å…¥åµŒå…¥å±‚æ·»åŠ ä¸€ä¸ª `forward_hook`ï¼Œä»¥å¯ç”¨è¾“å…¥éšè—çŠ¶æ€çš„æ¢¯åº¦è®¡ç®—\n",
    "- å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥å®ç°æ›´é«˜æ•ˆçš„å†…å­˜è®­ç»ƒ\n",
    "\n",
    "ä½¿ç”¨ `peft` åº“é¢„å®šä¹‰çš„å·¥å…·å‡½æ•° `prepare_model_for_int8_training`ï¼Œä¾¿å¯è‡ªåŠ¨å®Œæˆä»¥ä¸Šæ¨¡å‹å¤„ç†å·¥ä½œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ee34359-fe1b-48f1-827c-6a8ec4a53af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\peft\\utils\\other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_int8_training\n",
    "\n",
    "model = prepare_model_for_int8_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1212ae-c18c-459b-97a2-4b833c8414ae",
   "metadata": {},
   "source": [
    "### LoRA Adapter é…ç½®\n",
    "\n",
    "åœ¨ `peft` ä¸­ä½¿ç”¨`LoRA`éå¸¸ç®€æ·ï¼Œå€ŸåŠ© `PeftModel`æŠ½è±¡ï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€Ÿä½¿ç”¨ä½ç§©é€‚é…å™¨ï¼ˆLoRAï¼‰åˆ°ä»»æ„æ¨¡å‹ã€‚\n",
    "\n",
    "é€šè¿‡ä½¿ç”¨ `peft` ä¸­çš„ `get_peft_model` å·¥å…·å‡½æ•°æ¥å®ç°ã€‚\n",
    "\n",
    "#### å…³äº LoRA è¶…å‚æ•°çš„è¯´æ˜ï¼š\n",
    "```\n",
    "MatMul(B,A) * Scaling\n",
    "Scaling = LoRA_Alpha / Rank\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdf6bc9c-6d2c-4dbf-b09e-a89cb1041c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False, r=4, target_modules={'q_proj', 'v_proj'}, lora_alpha=64, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={})\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªLoraConfigå¯¹è±¡ï¼Œç”¨äºè®¾ç½®LoRAï¼ˆLow-Rank Adaptationï¼‰çš„é…ç½®å‚æ•°\n",
    "config = LoraConfig(\n",
    "    r=4,  # LoRAçš„ç§©ï¼Œå½±å“LoRAçŸ©é˜µçš„å¤§å°\n",
    "    lora_alpha=64,  # LoRAé€‚åº”çš„æ¯”ä¾‹å› å­\n",
    "    # æŒ‡å®šå°†LoRAåº”ç”¨åˆ°çš„æ¨¡å‹æ¨¡å—ï¼Œé€šå¸¸æ˜¯attentionå’Œå…¨è¿æ¥å±‚çš„æŠ•å½±ã€‚\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,  # åœ¨LoRAæ¨¡å—ä¸­ä½¿ç”¨çš„dropoutç‡\n",
    "    bias=\"none\",  # è®¾ç½®biasçš„ä½¿ç”¨æ–¹å¼ï¼Œè¿™é‡Œæ²¡æœ‰ä½¿ç”¨bias\n",
    ")\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584652a6-cf07-49d3-a4ee-66f360441fc0",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨get_peft_modelå‡½æ•°å’Œç»™å®šçš„é…ç½®æ¥è·å–ä¸€ä¸ªPEFTæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a8f9dc5-6e15-4f16-9ac5-e7492356fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b74c7508-e6f4-42d8-8aaf-fe83c5977c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,966,080 || all params: 1,545,271,040 || trainable%: 0.12723204856023188\n"
     ]
    }
   ],
   "source": [
    "# æ‰“å° LoRA å¾®è°ƒè®­ç»ƒçš„æ¨¡å‹å‚æ•°\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6b26a-3e54-4a46-9b36-a048b40a37d7",
   "metadata": {},
   "source": [
    "## æ¨¡å‹è®­ç»ƒ\n",
    "\n",
    "#### Seq2SeqTrainingArguments è®­ç»ƒå‚æ•°\n",
    "\n",
    "**å…³äºè®¾ç½®è®­ç»ƒæ­¥æ•°å’Œè¯„ä¼°æ­¥æ•°**\n",
    "\n",
    "åŸºäº epochs è®¾ç½®ï¼š\n",
    "\n",
    "```python\n",
    "    num_train_epochs=3,  # è®­ç»ƒçš„æ€»è½®æ•°\n",
    "    evaluation_strategy=\"epoch\",  # è®¾ç½®è¯„ä¼°ç­–ç•¥ï¼Œè¿™é‡Œæ˜¯åœ¨æ¯ä¸ªepochç»“æŸæ—¶è¿›è¡Œè¯„ä¼°\n",
    "    warmup_steps=50,  # åœ¨è®­ç»ƒåˆæœŸå¢åŠ å­¦ä¹ ç‡çš„æ­¥æ•°ï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒ\n",
    "```\n",
    "\n",
    "åŸºäº steps è®¾ç½®ï¼š\n",
    "\n",
    "```python\n",
    "    max_steps=100, # è®­ç»ƒæ€»æ­¥æ•°\n",
    "    evaluation_strategy=\"steps\", \n",
    "    eval_steps=25, # è¯„ä¼°æ­¥æ•°\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11f259c8-dbcf-4a7f-bbb5-821ab104efee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqTrainingArguments(\n",
      "_n_gpu=2,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.EPOCH,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=128,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=['labels'],\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/whisper-large-v2-asr-int8\\runs\\Jun10_23-21-50_llmtrain520,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=models/whisper-large-v2-asr-int8,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=64,\n",
      "per_device_train_batch_size=64,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=models/whisper-large-v2-asr-int8,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# è®¾ç½®åºåˆ—åˆ°åºåˆ—æ¨¡å‹è®­ç»ƒçš„å‚æ•°\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_dir,  # æŒ‡å®šæ¨¡å‹è¾“å‡ºå’Œä¿å­˜çš„ç›®å½•\n",
    "    per_device_train_batch_size=batch_size,  # æ¯ä¸ªè®¾å¤‡ä¸Šçš„è®­ç»ƒæ‰¹é‡å¤§å°\n",
    "    learning_rate=1e-3,  # å­¦ä¹ ç‡\n",
    "    num_train_epochs=1,  # è®­ç»ƒçš„æ€»è½®æ•°\n",
    "    evaluation_strategy=\"epoch\",  # è®¾ç½®è¯„ä¼°ç­–ç•¥ï¼Œè¿™é‡Œæ˜¯åœ¨æ¯ä¸ªepochç»“æŸæ—¶è¿›è¡Œè¯„ä¼°\n",
    "    # warmup_steps=50,  # åœ¨è®­ç»ƒåˆæœŸå¢åŠ å­¦ä¹ ç‡çš„æ­¥æ•°ï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒ\n",
    "    # fp16=True,  # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œå¯ä»¥æé«˜è®­ç»ƒé€Ÿåº¦ï¼ŒåŒæ—¶å‡å°‘å†…å­˜ä½¿ç”¨\n",
    "    per_device_eval_batch_size=batch_size,  # æ¯ä¸ªè®¾å¤‡ä¸Šçš„è¯„ä¼°æ‰¹é‡å¤§å°\n",
    "    generation_max_length=128,  # ç”Ÿæˆä»»åŠ¡çš„æœ€å¤§é•¿åº¦\n",
    "    logging_steps=10,  # æŒ‡å®šæ—¥å¿—è®°å½•çš„æ­¥éª¤ï¼Œç”¨äºè·Ÿè¸ªè®­ç»ƒè¿›åº¦\n",
    "    remove_unused_columns=False,  # æ˜¯å¦åˆ é™¤ä¸ä½¿ç”¨çš„åˆ—ï¼Œä»¥å‡å°‘æ•°æ®å¤„ç†å¼€é”€\n",
    "    label_names=[\"labels\"],  # æŒ‡å®šæ ‡ç­¾åˆ—çš„åç§°ï¼Œç”¨äºè®­ç»ƒè¿‡ç¨‹ä¸­\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=25,\n",
    ")\n",
    "\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ee183-b16f-4313-97f6-0df6c0f5f467",
   "metadata": {},
   "source": [
    "### å®ä¾‹åŒ– Seq2SeqTrainer è®­ç»ƒå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8a52ed7-cae0-4aba-818e-87717430d908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=peft_model,\n",
    "    train_dataset=tokenized_common_voice[\"train\"],\n",
    "    eval_dataset=tokenized_common_voice[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "peft_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6973bed7-8f53-4d55-966c-f037941e5ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:691: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 11:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.502200</td>\n",
       "      <td>1.092443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=1.5021549224853517, metrics={'train_runtime': 724.3278, 'train_samples_per_second': 0.884, 'train_steps_per_second': 0.014, 'total_flos': 1.36064139264e+18, 'train_loss': 1.5021549224853517, 'epoch': 1.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620992c3-64f5-48f9-8e66-fdc5f6a27427",
   "metadata": {},
   "source": [
    "### ä¿å­˜ LoRA æ¨¡å‹(Adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53310565-7313-46a7-acf1-215970fd4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "412785f2-f66f-492a-b01a-06ca81d9aed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): WhisperForConditionalGeneration(\n",
       "      (model): WhisperModel(\n",
       "        (encoder): WhisperEncoder(\n",
       "          (conv1): Conv1d(80, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (embed_positions): Embedding(1500, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperEncoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): WhisperDecoder(\n",
       "          (embed_tokens): Embedding(51865, 1280, padding_idx=50257)\n",
       "          (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperDecoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=1280, out_features=51865, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe9611-eee5-462f-8cb8-fed86eec76e0",
   "metadata": {},
   "source": [
    "## æ¨¡å‹æ¨ç†ï¼ˆå¯èƒ½éœ€è¦é‡å¯ Notebookï¼‰\n",
    "\n",
    "**å†æ¬¡åŠ è½½æ¨¡å‹ä¼šé¢å¤–å ç”¨æ˜¾å­˜ï¼Œå¦‚æœæ˜¾å­˜å·²ç»è¾¾åˆ°ä¸Šé™ï¼Œå»ºè®®é‡å¯ Notebook åå†è¿›è¡Œä»¥ä¸‹æ“ä½œ**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd2890f5-2eb9-493d-b43b-266fb12c6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"models/whisper-large-v2-asr-int8\"\n",
    "\n",
    "language = \"Chinese (China)\"\n",
    "language_abbr = \"zh-CN\"\n",
    "language_decode = \"chinese\"\n",
    "task = \"transcribe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ad8bc8-420b-4a98-b83f-08303693221b",
   "metadata": {},
   "source": [
    "\n",
    "### ä½¿ç”¨ `PeftModel` åŠ è½½ LoRA å¾®è°ƒå Whisper æ¨¡å‹\n",
    "\n",
    "ä½¿ç”¨ `PeftConfig` åŠ è½½ LoRA Adapter é…ç½®å‚æ•°ï¼Œä½¿ç”¨ `PeftModel` åŠ è½½å¾®è°ƒå Whisper æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d7f3af5-af01-4c26-80e9-976686983178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq, AutoTokenizer, AutoProcessor\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(model_dir)\n",
    "\n",
    "base_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3686334-d8d1-4782-834b-187aa684fb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
    "processor = AutoProcessor.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
    "feature_extractor = processor.feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31e558c-0c7b-445c-8210-52bd04fc0dd7",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨ Pipeline API éƒ¨ç½²å¾®è°ƒå Whisper å®ç°ä¸­æ–‡è¯­éŸ³è¯†åˆ«ä»»åŠ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18181692-a143-44ee-b56c-e754d308e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio = \"data/audio/test_zh.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d494647-082c-4e48-9486-7945618ae679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutomaticSpeechRecognitionPipeline\n",
    "\n",
    "pipeline = AutomaticSpeechRecognitionPipeline(model=peft_model, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language_decode, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90da1707-9054-416f-b0b6-a6203f8d3285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    text = pipeline(test_audio, max_new_tokens=255)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89f49787-6ab4-4bc1-91b8-a1c104c9feaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'è¿™æ˜¯ä¸€æ®µæµ‹è¯•ç”¨äºWhisperLarge V2æ¨¡å‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æµ‹è¯•ã€‚'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0285dd19-229e-4241-b680-71e25ab51dde",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "1. ä½¿ç”¨å®Œæ•´çš„æ•°æ®é›†è®­ç»ƒï¼Œå¯¹æ¯” Train Loss å’Œ Validation Loss å˜åŒ–ã€‚è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨æµ‹è¯•é›†è¿›è¡Œæ¨¡å‹è¯„ä¼°.\n",
    "2. [Optional]ä½¿ç”¨å…¶ä»–è¯­ç§ï¼ˆå¦‚ï¼šå¾·è¯­ã€æ³•è¯­ç­‰ï¼‰çš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒè®­ç»ƒï¼Œå¹¶è¿›è¡Œæ¨¡å‹è¯„ä¼°æ¨¡å‹è¯„ä¼°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8801650e-6666-412a-981f-1f8933d5df55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence', 'input_features', 'labels'],\n",
      "        num_rows: 29056\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['audio', 'sentence', 'input_features', 'labels'],\n",
      "        num_rows: 10581\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_common_voice = common_voice.map(prepare_dataset)\n",
    "print(tokenized_common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03f8ba63-4073-45cf-871a-4b634fb676b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 11:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.552100</td>\n",
       "      <td>0.485433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=0.552142858505249, metrics={'train_runtime': 722.5153, 'train_samples_per_second': 0.886, 'train_steps_per_second': 0.014, 'total_flos': 1.36064139264e+18, 'train_loss': 0.552142858505249, 'epoch': 1.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_homework = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=peft_model,\n",
    "    train_dataset=tokenized_common_voice[\"train\"],\n",
    "    eval_dataset=tokenized_common_voice[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12eb595a-5fca-48c9-9d0a-0e7e347afc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e0f207d-95fa-44f1-9b50-c2a68f64bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = PeftModel.from_pretrained(base_model, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb43f46e-85a5-428d-b18b-0fc59a3e8b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'è¿™æ˜¯ä¸€æ®µæµ‹è¯•ï¼Œç”¨äºWhisperLarge V2æ¨¡å‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æµ‹è¯•ã€‚'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.cuda.amp.autocast():\n",
    "    text = pipeline(test_audio, max_new_tokens=255)[\"text\"]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e7c5fe-c76c-4d16-b7b8-e4eeadcd8b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
