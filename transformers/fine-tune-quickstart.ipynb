{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c6730f-5d76-450b-9788-ec883d024f57",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers å¾®è°ƒè®­ç»ƒå…¥é—¨\n",
    "\n",
    "æœ¬ç¤ºä¾‹å°†ä»‹ç»åŸºäº Transformers å®ç°æ¨¡å‹å¾®è°ƒè®­ç»ƒçš„ä¸»è¦æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š\n",
    "- æ•°æ®é›†ä¸‹è½½\n",
    "- æ•°æ®é¢„å¤„ç†\n",
    "- è®­ç»ƒè¶…å‚æ•°é…ç½®\n",
    "- è®­ç»ƒè¯„ä¼°æŒ‡æ ‡è®¾ç½®\n",
    "- è®­ç»ƒå™¨åŸºæœ¬ä»‹ç»\n",
    "- å®æˆ˜è®­ç»ƒ\n",
    "- æ¨¡å‹ä¿å­˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b1e12-1921-4438-8d5d-9760a629dcfe",
   "metadata": {},
   "source": [
    "## YelpReviewFull æ•°æ®é›†\n",
    "\n",
    "**Hugging Face æ•°æ®é›†ï¼š[ YelpReviewFull ](https://huggingface.co/datasets/yelp_review_full)**\n",
    "\n",
    "### æ•°æ®é›†æ‘˜è¦\n",
    "\n",
    "Yelpè¯„è®ºæ•°æ®é›†åŒ…æ‹¬æ¥è‡ªYelpçš„è¯„è®ºã€‚å®ƒæ˜¯ä»Yelp Dataset Challenge 2015æ•°æ®ä¸­æå–çš„ã€‚\n",
    "\n",
    "### æ”¯æŒçš„ä»»åŠ¡å’Œæ’è¡Œæ¦œ\n",
    "æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†ç±»ï¼šè¯¥æ•°æ®é›†ä¸»è¦ç”¨äºæ–‡æœ¬åˆ†ç±»ï¼šç»™å®šæ–‡æœ¬ï¼Œé¢„æµ‹æƒ…æ„Ÿã€‚\n",
    "\n",
    "### è¯­è¨€\n",
    "è¿™äº›è¯„è®ºä¸»è¦ä»¥è‹±è¯­ç¼–å†™ã€‚\n",
    "\n",
    "### æ•°æ®é›†ç»“æ„\n",
    "\n",
    "#### æ•°æ®å®ä¾‹\n",
    "ä¸€ä¸ªå…¸å‹çš„æ•°æ®ç‚¹åŒ…æ‹¬æ–‡æœ¬å’Œç›¸åº”çš„æ ‡ç­¾ã€‚\n",
    "\n",
    "æ¥è‡ªYelpReviewFullæµ‹è¯•é›†çš„ç¤ºä¾‹å¦‚ä¸‹ï¼š\n",
    "\n",
    "```json\n",
    "{\n",
    "    'label': 0,\n",
    "    'text': 'I got \\'new\\' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\\\nI took the tire over to Flynn\\'s and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he\\'d give me a new tire \\\\\"this time\\\\\". \\\\nI will never go back to Flynn\\'s b/c of the way this guy treated me and the simple fact that they gave me a used tire!'\n",
    "}\n",
    "```\n",
    "\n",
    "#### æ•°æ®å­—æ®µ\n",
    "\n",
    "- 'text': è¯„è®ºæ–‡æœ¬ä½¿ç”¨åŒå¼•å·ï¼ˆ\"ï¼‰è½¬ä¹‰ï¼Œä»»ä½•å†…éƒ¨åŒå¼•å·éƒ½é€šè¿‡2ä¸ªåŒå¼•å·ï¼ˆ\"\"ï¼‰è½¬ä¹‰ã€‚æ¢è¡Œç¬¦ä½¿ç”¨åæ–œæ åè·Ÿä¸€ä¸ª \"n\" å­—ç¬¦è½¬ä¹‰ï¼Œå³ \"\\n\"ã€‚\n",
    "- 'label': å¯¹åº”äºè¯„è®ºçš„åˆ†æ•°ï¼ˆä»‹äº1å’Œ5ä¹‹é—´ï¼‰ã€‚\n",
    "\n",
    "#### æ•°æ®æ‹†åˆ†\n",
    "\n",
    "Yelpè¯„è®ºå®Œæ•´æ˜Ÿçº§æ•°æ®é›†æ˜¯é€šè¿‡éšæœºé€‰å–æ¯ä¸ª1åˆ°5æ˜Ÿè¯„è®ºçš„130,000ä¸ªè®­ç»ƒæ ·æœ¬å’Œ10,000ä¸ªæµ‹è¯•æ ·æœ¬æ„å»ºçš„ã€‚æ€»å…±æœ‰650,000ä¸ªè®­ç»ƒæ ·æœ¬å’Œ50,000ä¸ªæµ‹è¯•æ ·æœ¬ã€‚\n",
    "\n",
    "## ä¸‹è½½æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f7f968f-1181-4f74-9af7-6412d83f4c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "GPU device name: Tesla T4\n",
      "Number of GPUs available: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦å¯ä»¥è®¿é—® CUDA\n",
    "print(\"CUDA is available:\", torch.cuda.is_available()) # Should return True\n",
    "#print(\"torch nccl is available:\", torch.cuda.nccl.is_available())  # Should return True\n",
    "\n",
    "# æŸ¥çœ‹å½“å‰ä½¿ç”¨çš„ GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU device name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "else:\n",
    "    print(\"No GPU available, using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf72d6c-7ea5-4ee1-969a-c5060b9cb2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6fc806-1395-42dd-8121-a6e98a95cf01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94ad529-1604-48bd-8c8d-aa2f3bca6200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 2,\n",
       " 'text': \"As far as Starbucks go, this is a pretty nice one.  The baristas are friendly and while I was here, a lot of regulars must have come in, because they bantered away with almost everyone.  The bathroom was clean and well maintained and the trash wasn't overflowing in the canisters around the store.  The pastries looked fresh, but I didn't partake.  The noise level was also at a nice working level - not too loud, music just barely audible.\\\\n\\\\nI do wish there was more seating.  It is nice that this location has a counter at the end of the bar for sole workers, but it doesn't replace more tables.  I'm sure this isn't as much of a problem in the summer when there's the space outside.\\\\n\\\\nThere was a treat receipt promo going on, but the barista didn't tell me about it, which I found odd.  Usually when they have promos like that going on, they ask everyone if they want their receipt to come back later in the day to claim whatever the offer is.  Today it was one of their new pastries for $1, I know in the summer they do $2 grande iced drinks with that morning's receipt.\\\\n\\\\nOverall, nice working or socializing environment.  Very friendly and inviting.  It's what I've come to expect from Starbucks, so points for consistency.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc45997-e391-456f-b0b9-d3193b0f6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e2ecebb-d5d1-456d-967c-842a79fdd622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1af560b6-7d21-499e-9b82-114be371a98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>First real food stop here in Vegas after being up nearly 24 hours. I am in Vegas for the Mr. Olympia expo and after seeing all the healthy fit people one of the people in our group wanted to eat at this fattening restaurant. I had never been here, but was very hungry after walking all day, and being off my diet for over 8 months. Great way to celebrate with this high calorie restaurant. \\n\\nThe first thing you notice in front of the restaurant is a scale...yes to weigh yourself. Apparently they promote being oversized, since all those over 350 lbs. eat free!! Our whole group weighed ourselves just to see how much we would weigh after eating here. Upon entering we were greeted by nurses and given a hospital patient gown. It was the best how very themed the restaurant was getting all customers into their world. This created a wonderful atmosphere for us to lose ourselves and enjoy the moment. They are known for the highest calorie burger with the quadruple bypass burger. It has 4 1/2lb. patties with cheese chili, onion tomato and pickles. Basically also everything is cooked in lard to raise the calories even more! The burger alone was 8000 calories. I added fries also cooked in lard, so who knows how much more calories that added! I also got a strawberry daiquiri that came with a personalized prescription bottle with your name printed on it. The bottle was 3.4 oz of a shot of alcohol. This was a heavy, high calorie meal that I ate completely but felt guilty and shame at the same time. \\n\\nEverything tasted good, but it was all about the experience of the restaurant and how the theme of it enthralls you while eating there. If you don't manage to finish your burger, you will get spanked. Yes, spanked by one of the nurses with a wooden paddle. Very funny to see when it happened. I would come back here again, but only with friends. The reason why is to enjoy the experience. I didn't think the food was all that great, but still good. \\n\\nI would recommend this place to all first timers and all those who love burgers. I'm really giving it 3 stars for the food, and the extra star for the theme given at the restaurant. Hope this helps you decide.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>I have always been a Four Peaks fan - the beer AND the grub. It's hard for me to ever order anything other than the Taproom Tenderloin Sandwich when I eat there.  Best.sandwich.ever. I've always enjoyed the Kilt Lifter and the 8th Street Ale beer, but after going there last week, my new favorite is the Hop Knot IPA. I'll be going back soon just for that! I have fun memories of happy hours back in the day at the original Four Peaks in Tempe and while I enjoy going to the location in Scottsdale, too, there's something authentic about going to the one in Tempe. I love the true brewery atmosphere and that it's somewhat hidden, tucked back off major streets. Always a winner!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>I end up at Gluuteny every single time I visit Squirrel Hill because I want to love this bakery, and now that I've tried almost all their offerings in the way of prepared goods I can safely say, \\\"It's okay.\\\"  \\n\\nTheir brownies ($2.50) are the best thing they make in my book: they're better than most \\\"normal\\\" brownies. So good they don't taste gluten-free? Hell yeah, I'll make that claim!\\n\\nThe pumpkin and lemon loafs ($5) are addictive: I ate one small loaf in a day...by myself...and then I ate another one...don't judge me! The pumpkin tart ($2.50) is also really good.\\n\\nThe muffins ($2) come in blueberry, orange cranberry and chocolate chip, and are on the spongy side but their flavor is spot on. \\n\\nThe cookies ($2), however, aren't done right; the sugar cookies are slathered in what can only be described as a suspicious amount of frosting. The other varieties, like chocolate chip and walnut chocolate chip, are just too crunchy to be enjoyable. What can I say? I like soft cookies.\\n\\nIf you're gluten-free this is a good place to stock up on pizza crust and pancake mixes, bread, muffins or any other pastry mix your heart desires; the lady in front of me bought fifty bucks worth of stuff!\\n\\nThe girls behind the counter that rock some of the cutest haircuts in all of PGH are willing to answer any questions or explain that you need to cook the pizza at a higher temperature to get the crust the right texture. Plus, you can order birthday cakes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 star</td>\n",
       "      <td>Very disappointing. I ordered the \\\"Pigs In A Bagel\\\" (Two eggs, peppered bacon &amp; tomato topped with cheddar, served on your choice of bagel with herb potato &amp; fresh fruit) and I also asked the waitress \\\"how do the EGGS come?\\\" And she said \\\"scrambled\\\" so I just thought it was going to be a bagel, with scrambled eggs, slices of bacon on top, tomato, and topped with the cheddar cheese. I get my \\\"sandwich\\\" and it's a little squished bagel with a gross omelette inside with tiny bits of bacon and tomato, like a drop of some white cheese, and some wilted lettuce, really un satisfying. The booth we were sitting at was so freezing that our food was cold within 5 minutes. The manager offered to get me something else so I just asked for a mixed berry smoothie to  go to play it safe. I thought it tasted kind of milky so I opened the lid and there was a huge pile of whip cream, yuck. Since when do breakfast sandwiches come with lettuce and smoothies come with whipped cream? Very strange. Our waitress also seemed like she could care less about us and I think we were there for almost 2 hours, super slow and ridiculous. Don't think I'll be going back there.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 star</td>\n",
       "      <td>these guys kept my engine for 4 months and never delivered on their promises. search the 240 forums and ripoffreport.com  you'll see what others are talking about. In the end this place ended up costing me 5 months of delays and around $1200 in additional costs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>Good food and great service.  They have a tasty salad and a good beer selection.  The pizzas are fantastic.  I have tried each appetizer and enjoyed them all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1 star</td>\n",
       "      <td>Called ahead to check price on international shipping. After quote I headed in and dealt with Andrew. He was unable to process the shipment stating it was not allowed to be sent to the country. I was shipping paper. He said there was a glitch in the system but refused to get on the phone with anyone or call his manager. He told me to go to UPS. I went to the fed ex on power rd in Mesa and they shipped it just fine. Incompetent. No customer service. Complete waste of time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Located in the Rivers Casino with a view of the patio and river. I can tell that outside seating in the summer is tops here. This was a good stop for food before gaming. \\n\\nThe staff was friendly to us but painfully slow to get hamburgers, salad and chicken fingers order out to us! Though, I bet that was fine, so you do not spend all your cash in the casino while your eating at least. ;) \\n\\nDrinks are overpriced and food was very average. Burgers were ok but the fires were dried out and unappealing.\\n\\nThe chop salad with grilled chix, however, was the best meal on the table. I'd given 2 stars if my mother was not raving how much she enjoyed said salad.\\n\\nI'd say worth a try esp if you can hit the happy hours here and are into watching sports, they have a million tv's in here. ;).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2 star</td>\n",
       "      <td>Only come here if you don't care about how you smell afterward. \\nAfter you eat here, there is a strong odour that comes with you afterward. \\nTheir food tasted okay, but I heard people had diarrhea afterward, maybe their food is not that fresh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1 star</td>\n",
       "      <td>I had a similar experience as most other reviewers.  I bought a Groupon, made an appointment, had to reschedule my appointment and never got a call back.  The professionalism is severely lacking.  Groupon was very accommodating when I made my complaint against this business.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df7cd0-23cd-458f-b2b5-f025c3b9fe62",
   "metadata": {},
   "source": [
    "## é¢„å¤„ç†æ•°æ®\n",
    "\n",
    "ä¸‹è½½æ•°æ®é›†åˆ°æœ¬åœ°åï¼Œä½¿ç”¨ Tokenizer æ¥å¤„ç†æ–‡æœ¬ï¼Œå¯¹äºé•¿åº¦ä¸ç­‰çš„è¾“å…¥æ•°æ®ï¼Œå¯ä»¥ä½¿ç”¨å¡«å……ï¼ˆpaddingï¼‰å’Œæˆªæ–­ï¼ˆtruncationï¼‰ç­–ç•¥æ¥å¤„ç†ã€‚\n",
    "\n",
    "Datasets çš„ `map` æ–¹æ³•ï¼Œæ”¯æŒä¸€æ¬¡æ€§åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†å‡½æ•°ã€‚\n",
    "\n",
    "ä¸‹é¢ä½¿ç”¨å¡«å……åˆ°æœ€å¤§é•¿åº¦çš„ç­–ç•¥ï¼Œå¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bf2b342-e1dd-4ab6-ad57-28eb2513ae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a415a8-cd15-4a8c-851b-9b4740ef8271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>When we got there at 5:45, it was packed!  One high top table available with two chairs.  After checking in for 3, the hostess kindly put a chair up to the high top to accommodate us!\\n\\nWaiter Alex was amazing - food amazing!  Our absolute FAVORITE Mexican restaurant in Scottsdale.  Oh, and btw - Sweetheart Margarita was amazing - had to order a second!!</td>\n",
       "      <td>[101, 1332, 1195, 1400, 1175, 1120, 126, 131, 2532, 117, 1122, 1108, 8733, 106, 1448, 1344, 1499, 1952, 1907, 1114, 1160, 8391, 119, 1258, 9444, 1107, 1111, 124, 117, 1103, 28053, 21559, 1508, 170, 2643, 1146, 1106, 1103, 1344, 1499, 1106, 8378, 1366, 106, 165, 183, 165, 183, 2924, 21263, 1200, 3230, 1108, 6929, 118, 2094, 6929, 106, 3458, 7846, 6820, 21049, 20595, 12880, 4112, 4382, 1107, 2796, 20537, 119, 2048, 117, 1105, 171, 1204, 2246, 118, 7643, 19233, 9751, 5526, 5168, 1108, 6929, 118, 1125, 1106, 1546, 170, 1248, 106, 106, 102, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(tokenized_datasets[\"train\"], num_examples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c33d153-f729-4f04-972c-a764c1cbbb8b",
   "metadata": {},
   "source": [
    "### æ•°æ®æŠ½æ ·\n",
    "\n",
    "ä½¿ç”¨ 1000 ä¸ªæ•°æ®æ ·æœ¬ï¼Œåœ¨ BERT ä¸Šæ¼”ç¤ºå°è§„æ¨¡è®­ç»ƒï¼ˆåŸºäº Pytorch Trainerï¼‰\n",
    "\n",
    "`shuffle()`å‡½æ•°ä¼šéšæœºé‡æ–°æ’åˆ—åˆ—çš„å€¼ã€‚å¦‚æœæ‚¨å¸Œæœ›å¯¹ç”¨äºæ´—ç‰Œæ•°æ®é›†çš„ç®—æ³•æœ‰æ›´å¤šæ§åˆ¶ï¼Œå¯ä»¥åœ¨æ­¤å‡½æ•°ä¸­æŒ‡å®šgeneratorå‚æ•°æ¥ä½¿ç”¨ä¸åŒçš„numpy.random.Generatorã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a17317d8-3c6a-467f-843d-87491f600db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "print(len(small_train_dataset))\n",
    "print(len(small_eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b65d63-2d3a-4a56-bc31-6e88a29e9dec",
   "metadata": {},
   "source": [
    "## å¾®è°ƒè®­ç»ƒé…ç½®\n",
    "\n",
    "### åŠ è½½ BERT æ¨¡å‹\n",
    "\n",
    "è­¦å‘Šé€šçŸ¥æˆ‘ä»¬æ­£åœ¨ä¸¢å¼ƒä¸€äº›æƒé‡ï¼ˆ`vocab_transform` å’Œ `vocab_layer_norm` å±‚ï¼‰ï¼Œå¹¶éšæœºåˆå§‹åŒ–å…¶ä»–ä¸€äº›æƒé‡ï¼ˆ`pre_classifier` å’Œ `classifier` å±‚ï¼‰ã€‚åœ¨å¾®è°ƒæ¨¡å‹æƒ…å†µä¸‹æ˜¯ç»å¯¹æ­£å¸¸çš„ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨åˆ é™¤ç”¨äºé¢„è®­ç»ƒæ¨¡å‹çš„æ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„å¤´éƒ¨ï¼Œå¹¶ç”¨ä¸€ä¸ªæ–°çš„å¤´éƒ¨æ›¿æ¢å®ƒï¼Œå¯¹äºè¿™ä¸ªæ–°å¤´éƒ¨ï¼Œæˆ‘ä»¬æ²¡æœ‰é¢„è®­ç»ƒçš„æƒé‡ï¼Œæ‰€ä»¥åº“ä¼šè­¦å‘Šæˆ‘ä»¬åœ¨ç”¨å®ƒè¿›è¡Œæ¨ç†ä¹‹å‰åº”è¯¥å¯¹è¿™ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè€Œè¿™æ­£æ˜¯æˆ‘ä»¬è¦åšçš„äº‹æƒ…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d2af4df-abd4-4a4b-94b6-b0e7375304ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44014df-b52c-4c72-9e9f-54424725a473",
   "metadata": {},
   "source": [
    "### è®­ç»ƒè¶…å‚æ•°ï¼ˆTrainingArgumentsï¼‰\n",
    "\n",
    "å®Œæ•´é…ç½®å‚æ•°ä¸é»˜è®¤å€¼ï¼šhttps://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "\n",
    "æºä»£ç å®šä¹‰ï¼šhttps://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161\n",
    "\n",
    "**æœ€é‡è¦é…ç½®ï¼šæ¨¡å‹æƒé‡ä¿å­˜è·¯å¾„(output_dir)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98c01d5c-de72-4ff0-b11d-e07ac5346888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "model_dir = \"models/bert-base-cased-finetune-yelp\"\n",
    "\n",
    "# logging_steps é»˜è®¤å€¼ä¸º500ï¼Œæ ¹æ®æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®å’Œæ­¥é•¿ï¼Œå°†å…¶è®¾ç½®ä¸º100\n",
    "training_args = TrainingArguments(output_dir=model_dir,\n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  num_train_epochs=5,\n",
    "                                  logging_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ce03480-3aaa-48ea-a0c6-a177b8d8e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=2,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/bert-base-cased-finetune-yelp\\runs\\Jun06_11-42-16_llmtrain520,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=models/bert-base-cased-finetune-yelp,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=models/bert-base-cased-finetune-yelp,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# å®Œæ•´çš„è¶…å‚æ•°é…ç½®\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebd3365-d359-4ab4-a300-4717590cc240",
   "metadata": {},
   "source": [
    "### è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡æ ‡è¯„ä¼°ï¼ˆEvaluate)\n",
    "\n",
    "**[Hugging Face Evaluate åº“](https://huggingface.co/docs/evaluate/index)** æ”¯æŒä½¿ç”¨ä¸€è¡Œä»£ç ï¼Œè·å¾—æ•°åç§ä¸åŒé¢†åŸŸï¼ˆè‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€å¼ºåŒ–å­¦ä¹ ç­‰ï¼‰çš„è¯„ä¼°æ–¹æ³•ã€‚ å½“å‰æ”¯æŒ **å®Œæ•´è¯„ä¼°æŒ‡æ ‡ï¼šhttps://huggingface.co/evaluate-metric**\n",
    "\n",
    "è®­ç»ƒå™¨ï¼ˆTrainerï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¼šè‡ªåŠ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å‘è®­ç»ƒå™¨ä¼ é€’ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—å’ŒæŠ¥å‘ŠæŒ‡æ ‡ã€‚ \n",
    "\n",
    "Evaluateåº“æä¾›äº†ä¸€ä¸ªç®€å•çš„å‡†ç¡®ç‡å‡½æ•°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`evaluate.load`å‡½æ•°åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a8ef138-5bf2-41e5-8c68-df8e11f4e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d406c0-56d0-4a54-9c6c-e126ab7f5254",
   "metadata": {},
   "source": [
    "\n",
    "æ¥ç€ï¼Œè°ƒç”¨ `compute` å‡½æ•°æ¥è®¡ç®—é¢„æµ‹çš„å‡†ç¡®ç‡ã€‚\n",
    "\n",
    "åœ¨å°†é¢„æµ‹ä¼ é€’ç»™ compute å‡½æ•°ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å°† logits è½¬æ¢ä¸ºé¢„æµ‹å€¼ï¼ˆ**æ‰€æœ‰Transformers æ¨¡å‹éƒ½è¿”å› logits**ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f46d2e59-1ebf-43d2-bc86-6b57a4d24d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2feba67-9ca9-4793-9a15-3eaa426df2a1",
   "metadata": {},
   "source": [
    "#### è®­ç»ƒè¿‡ç¨‹æŒ‡æ ‡ç›‘æ§\n",
    "\n",
    "é€šå¸¸ï¼Œä¸ºäº†ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¯„ä¼°æŒ‡æ ‡å˜åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨`TrainingArguments`æŒ‡å®š`evaluation_strategy`å‚æ•°ï¼Œä»¥ä¾¿åœ¨ epoch ç»“æŸæ—¶æŠ¥å‘Šè¯„ä¼°æŒ‡æ ‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afaaee18-4986-4e39-8ad9-b8d413ab4cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "# å› ä¸ºGPUé…ç½®ä½ï¼Œå°†per_device_train_batch_sizeç”±16æ”¹ä¸º8ï¼Œå¦åˆ™è®­ç»ƒæ—¶æŠ¥å†…å­˜æº¢å‡ºé”™è¯¯\n",
    "training_args = TrainingArguments(output_dir=model_dir,\n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  per_device_train_batch_size=24,\n",
    "                                  num_train_epochs=3,\n",
    "                                  logging_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d6981-e444-4c0f-a7cb-dd7f2ba8df12",
   "metadata": {},
   "source": [
    "## å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "### å®ä¾‹åŒ–è®­ç»ƒå™¨ï¼ˆTrainerï¼‰\n",
    "\n",
    "`kernel version` ç‰ˆæœ¬é—®é¢˜ï¼šæš‚ä¸å½±å“æœ¬ç¤ºä¾‹ä»£ç è¿è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca1d12ac-89dc-4c30-8282-f859724c0062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833e0db-1168-4a3c-8b75-bfdcef8c5157",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ nvidia-smi æŸ¥çœ‹ GPU ä½¿ç”¨\n",
    "\n",
    "ä¸ºäº†å®æ—¶æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µï¼Œå¯ä»¥ä½¿ç”¨ `watch` æŒ‡ä»¤å®ç°è½®è¯¢ï¼š`watch -n 1 nvidia-smi`:\n",
    "\n",
    "```shell\n",
    "Every 1.0s: nvidia-smi                                                   Wed Dec 20 14:37:41 2023\n",
    "\n",
    "Wed Dec 20 14:37:41 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   64C    P0              69W /  70W |   6665MiB / 15360MiB |     98%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     18395      C   /root/miniconda3/bin/python                6660MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "accfe921-471d-481a-96da-c491cdebad0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 03:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.318491</td>\n",
       "      <td>0.498000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.429200</td>\n",
       "      <td>1.067790</td>\n",
       "      <td>0.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.978500</td>\n",
       "      <td>0.984592</td>\n",
       "      <td>0.590000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=1.1894795516180614, metrics={'train_runtime': 195.218, 'train_samples_per_second': 15.367, 'train_steps_per_second': 0.323, 'total_flos': 789354427392000.0, 'train_loss': 1.1894795516180614, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d581099-37a4-4470-b051-1ada38554089",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=64).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffb47eab-1370-491e-8a84-6d5347a350b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0356402397155762,\n",
       " 'eval_accuracy': 0.55,\n",
       " 'eval_runtime': 1.9388,\n",
       " 'eval_samples_per_second': 51.579,\n",
       " 'eval_steps_per_second': 3.611,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(small_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a55686-7c43-4ab8-a5cd-0e77f14c7c52",
   "metadata": {},
   "source": [
    "### ä¿å­˜æ¨¡å‹å’Œè®­ç»ƒçŠ¶æ€\n",
    "\n",
    "- ä½¿ç”¨ `trainer.save_model` æ–¹æ³•ä¿å­˜æ¨¡å‹ï¼Œåç»­å¯ä»¥é€šè¿‡ from_pretrained() æ–¹æ³•é‡æ–°åŠ è½½\n",
    "- ä½¿ç”¨ `trainer.save_state` æ–¹æ³•ä¿å­˜è®­ç»ƒçŠ¶æ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad0cbc14-9ef7-450f-a1a3-4f92b6486f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e30510-0536-49d4-8e1b-43fc25272bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "badf5868-2847-439d-a73e-42d1cca67b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9441ad-f65a-42b7-9016-4809c78285e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd92e35d-fed7-4ff2-aa84-27b5e29b917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.model.save_pretrained(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61828934-01da-4fc3-9e75-8d754c25dfbc",
   "metadata": {},
   "source": [
    "## Homework: ä½¿ç”¨å®Œæ•´çš„ YelpReviewFull æ•°æ®é›†è®­ç»ƒï¼Œçœ‹ Acc æœ€é«˜èƒ½åˆ°å¤šå°‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d005449-884d-464d-afcc-f25ee535b8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650000\n"
     ]
    }
   ],
   "source": [
    "# æ‰“å°æ•´ä¸ªæ•°æ®é›†çš„å¤§å°\n",
    "print(len(tokenized_datasets[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed6896bb-c45e-465d-8bd5-bf5ebd7de325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130000\n"
     ]
    }
   ],
   "source": [
    "# å› ä¸ºGPUé…ç½®ä½ï¼Œæ‰€ä»¥ä¸è®­ç»ƒå…¨é‡æ•°æ®ï¼Œæ”¹ä¸ºè®­ç»ƒ1/5çš„æ•°æ®é‡\n",
    "# è®¡ç®—æ•°æ®é›†å¤§å°\n",
    "total_train_size = len(tokenized_datasets[\"train\"])\n",
    "total_eval_size = len(tokenized_datasets[\"test\"])\n",
    "\n",
    "# è®¡ç®—1/5çš„å¤§å°\n",
    "half_train_size = total_train_size // 5\n",
    "half_eval_size = total_eval_size // 5\n",
    "\n",
    "# é€‰æ‹©è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å‰ä¸€åŠæ•°æ®\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(half_train_size))\n",
    "eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(half_eval_size))\n",
    "\n",
    "#train_dataset = tokenized_datasets[\"train\"]\n",
    "#eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a474a29b-26c0-4e3f-bdad-bfe2bd9614dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=2,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.EPOCH,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/bert-base-cased-finetune-yelp\\runs\\Jun06_11-45-51_llmtrain520,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1000,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=models/bert-base-cased-finetune-yelp,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=24,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=models/bert-base-cased-finetune-yelp,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# å› ä¸ºGPUé…ç½®ä½ï¼Œå°†per_device_train_batch_sizeç”±16æ”¹ä¸º8ï¼Œå¦åˆ™è®­ç»ƒæ—¶æŠ¥å†…å­˜æº¢å‡ºé”™è¯¯\n",
    "training_args = TrainingArguments(output_dir=model_dir,\n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  per_device_train_batch_size=24,\n",
    "                                  num_train_epochs=3,\n",
    "                                  logging_steps=1000)\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ee2580a-7a5a-46ae-a28b-b41e9e838eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8127' max='8127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8127/8127 5:20:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.819700</td>\n",
       "      <td>0.779540</td>\n",
       "      <td>0.654200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.651800</td>\n",
       "      <td>0.786093</td>\n",
       "      <td>0.668300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.483700</td>\n",
       "      <td>0.872761</td>\n",
       "      <td>0.665700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8127, training_loss=0.6648250254549721, metrics={'train_runtime': 19234.7041, 'train_samples_per_second': 20.276, 'train_steps_per_second': 0.423, 'total_flos': 1.0261607556096e+17, 'train_loss': 0.6648250254549721, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_homework = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_homework.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73e37d45-cd35-41a4-876b-06728efdbe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdgc\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 06:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8580308556556702,\n",
       " 'eval_accuracy': 0.6724,\n",
       " 'eval_runtime': 407.198,\n",
       " 'eval_samples_per_second': 49.116,\n",
       " 'eval_steps_per_second': 3.07,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = tokenized_datasets[\"test\"].shuffle(seed=64).select(range(20000))\n",
    "trainer_homework.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0cc0475-4f08-46a4-8140-f109d6af81ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_homework.save_model(model_dir)\n",
    "trainer_homework.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418be325-cf35-446f-b5ad-8dd6be4b8c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
